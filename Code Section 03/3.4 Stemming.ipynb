{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n"
     ]
    }
   ],
   "source": [
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous --> generous\n",
      "generous --> gener\n",
      "---------------------------------------\n",
      "generation --> generat\n",
      "generation --> gener\n",
      "---------------------------------------\n",
      "generously --> generous\n",
      "generously --> gener\n",
      "---------------------------------------\n",
      "generate --> generat\n",
      "generate --> gener\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "words = ['generous','generation','generously','generate']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))\n",
    "    print(word+' --> '+p_stemmer.stem(word))\n",
    "    print('---------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer , LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "ls =  LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"is\",\"was\",\"be\",\"been\",\"are\",\"were\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  generous    has setmming      gener\n",
      "Word  generation    has setmming      gener\n",
      "Word  generously    has setmming      gener\n",
      "Word  generate    has setmming      gener\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(f'Word  {w}    has setmming      {ps.stem(w)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  generous    has setmming      gen\n",
      "Word  generation    has setmming      gen\n",
      "Word  generously    has setmming      gen\n",
      "Word  generate    has setmming      gen\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(f'Word  {w}    has setmming      {ls.stem(w)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  book    has setmming      book\n",
      "Word  booking    has setmming      book\n",
      "Word  booked    has setmming      book\n",
      "Word  books    has setmming      book\n",
      "Word  booker    has setmming      booker\n",
      "Word  bookstore    has setmming      bookstor\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(f'Word  {w}    has setmming      {s_stemmer.stem(w)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"book\",\"booking\",\"booked\",\"books\",\"booker\",\"bookstore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  book    has setmming      book\n",
      "Word  booking    has setmming      book\n",
      "Word  booked    has setmming      book\n",
      "Word  books    has setmming      book\n",
      "Word  booker    has setmming      booker\n",
      "Word  bookstore    has setmming      bookstor\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(f'Word  {w}    has setmming      {ps.stem(w)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  book    has setmming      book\n",
      "Word  booking    has setmming      book\n",
      "Word  booked    has setmming      book\n",
      "Word  books    has setmming      book\n",
      "Word  booker    has setmming      book\n",
      "Word  bookstore    has setmming      bookst\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(f'Word  {w}    has setmming      {ls.stem(w)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'had you booked the air booking yet ? if not try to book it ASAP since booking will be out of books'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  had    has setmming      had\n",
      "Word  you    has setmming      you\n",
      "Word  booked    has setmming      book\n",
      "Word  the    has setmming      the\n",
      "Word  air    has setmming      air\n",
      "Word  booking    has setmming      book\n",
      "Word  yet    has setmming      yet\n",
      "Word  ?    has setmming      ?\n",
      "Word  if    has setmming      if\n",
      "Word  not    has setmming      not\n",
      "Word  try    has setmming      tri\n",
      "Word  to    has setmming      to\n",
      "Word  book    has setmming      book\n",
      "Word  it    has setmming      it\n",
      "Word  ASAP    has setmming      asap\n",
      "Word  since    has setmming      sinc\n",
      "Word  booking    has setmming      book\n",
      "Word  will    has setmming      will\n",
      "Word  be    has setmming      be\n",
      "Word  out    has setmming      out\n",
      "Word  of    has setmming      of\n",
      "Word  books    has setmming      book\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(sentence)\n",
    "\n",
    "for w in words:\n",
    "    print(f'Word  {w}    has setmming      {ps.stem(w)}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  had    has setmming      had\n",
      "Word  you    has setmming      you\n",
      "Word  booked    has setmming      book\n",
      "Word  the    has setmming      the\n",
      "Word  air    has setmming      air\n",
      "Word  booking    has setmming      book\n",
      "Word  yet    has setmming      yet\n",
      "Word  ?    has setmming      ?\n",
      "Word  if    has setmming      if\n",
      "Word  not    has setmming      not\n",
      "Word  try    has setmming      try\n",
      "Word  to    has setmming      to\n",
      "Word  book    has setmming      book\n",
      "Word  it    has setmming      it\n",
      "Word  ASAP    has setmming      asap\n",
      "Word  since    has setmming      sint\n",
      "Word  booking    has setmming      book\n",
      "Word  will    has setmming      wil\n",
      "Word  be    has setmming      be\n",
      "Word  out    has setmming      out\n",
      "Word  of    has setmming      of\n",
      "Word  books    has setmming      book\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(sentence)\n",
    "\n",
    "for w in words:\n",
    "    print(f'Word  {w}    has setmming      {ls.stem(w)}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      lancaster Stemmer   \n",
      "friend              friend              friend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "stabil              stabil              stabl               \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\n",
    "             \"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,ps.stem(word),ls.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t 4690420944186131903 \t I\n",
      "am \t AUX \t 10382539506755952630 \t be\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "runner \t NOUN \t 12640964157389618806 \t runner\n",
      "running \t VERB \t 12767647472892411841 \t run\n",
      "in \t ADP \t 3002984154512732771 \t in\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "race \t NOUN \t 8048469955494714898 \t race\n",
      "because \t SCONJ \t 16950148841647037698 \t because\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "love \t VERB \t 3702023516439754181 \t love\n",
      "to \t PART \t 3791531372978436496 \t to\n",
      "run \t VERB \t 12767647472892411841 \t run\n",
      "since \t SCONJ \t 10066841407251338481 \t since\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "ran \t VERB \t 12767647472892411841 \t run\n",
      "yesterday \t NOUN \t 1756787072497230782 \t yesterday\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc1 = nlp(u\"I am a runner running in a race because I love to run since I ran yesterday\")\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_} {token.dep_} {spacy.explain(token.lemma_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   4690420944186131903    I nsubj None\n",
      "saw          VERB   11925638236994514241   see ROOT None\n",
      "eighteen     NUM    9609336664675087640    eighteen nummod None\n",
      "mice         NOUN   1384165645700560590    mouse dobj None\n",
      "today        NOUN   11042482332948150395   today npadvmod None\n",
      "!            PUNCT  17494803046312582752   ! punct None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\spacy\\glossary.py:20: UserWarning: [W118] Term 'I' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n",
      "c:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\spacy\\glossary.py:20: UserWarning: [W118] Term 'see' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n",
      "c:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\spacy\\glossary.py:20: UserWarning: [W118] Term 'eighteen' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n",
      "c:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\spacy\\glossary.py:20: UserWarning: [W118] Term 'mouse' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n",
      "c:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\spacy\\glossary.py:20: UserWarning: [W118] Term 'today' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n",
      "c:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\spacy\\glossary.py:20: UserWarning: [W118] Term '!' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"I saw eighteen mice today!\")\n",
    "\n",
    "show_lemmas(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   4690420944186131903    I\n",
      "am           AUX    10382539506755952630   be\n",
      "meeting      VERB   6880656908171229526    meet\n",
      "him          PRON   1655312771067108281    he\n",
      "right        ADV    5943797630011647483    right\n",
      "now          ADV    17157488710739566268   now\n",
      "at           ADP    11667289587015813222   at\n",
      "the          DET    7425985699627899538    the\n",
      "meeting      NOUN   14798207169164081740   meeting\n",
      ".            PUNCT  12646065887601541794   .\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(u\"I am meeting him right now at the meeting.\")\n",
    "\n",
    "show_lemmas(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "radius\n",
      "foot\n",
      "speech\n",
      "runner\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "words = [\"cats\",\"cacti\",\"radii\",\"feet\",\"speech\",'runner']\n",
    "\n",
    "for word in words : \n",
    "    print(lemmatizer.lemmatize(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meeting\n",
      "meet\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"meeting\", \"n\"))\n",
    "print(lemmatizer.lemmatize(\"meeting\",'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n",
      "wa\n",
      "be\n",
      "been\n",
      "are\n",
      "were\n"
     ]
    }
   ],
   "source": [
    "words = [\"is\",\"was\",\"be\",\"been\",\"are\",\"were\"]\n",
    "\n",
    "for word in words : \n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "be\n",
      "be\n",
      "be\n",
      "be\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "words = [\"is\",\"was\",\"be\",\"been\",\"are\",\"were\"]\n",
    "for word in words : \n",
    "    print(lemmatizer.lemmatize(word,'v'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foot\n",
      "radius\n",
      "men\n",
      "child\n",
      "carpenter\n",
      "fighter\n"
     ]
    }
   ],
   "source": [
    "words = [\"feet\",\"radii\",\"men\",\"children\",\"carpenter\",\"fighter\"]\n",
    "for word in words : \n",
    "    print(lemmatizer.lemmatize(word,'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الجري --> الجري\n",
      "تجري --> تجري\n",
      "يجرون --> يجرون\n",
      "جري --> جري\n",
      "يجري --> يجري\n"
     ]
    }
   ],
   "source": [
    "words = ['الجري','تجري','يجرون','جري','يجري']\n",
    "for word in words:\n",
    "    print(word+' --> '+p_stemmer.stem(word))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الجري --> الجر\n",
      "تجري --> تجر\n",
      "يجرون --> يجرو\n",
      "جري --> جر\n",
      "يجري --> يجر\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(language='arabic')\n",
    "\n",
    "words = ['الجري','تجري','يجرون','جري','يجري']\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الجري --> الجر\n",
      "تجري --> تجر\n",
      "يجرون --> يجرو\n",
      "جري --> جر\n",
      "يجري --> يجر\n"
     ]
    }
   ],
   "source": [
    "words = ['الجري','تجري','يجرون','جري','يجري']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer , LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "ls =  LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['الجري','تجري','يجرون','جري','يجري']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الجري\n",
      "تجري\n",
      "يجرون\n",
      "جري\n",
      "يجري\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الجري\n",
      "تجري\n",
      "يجرون\n",
      "جري\n",
      "يجري\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(ls.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      lancaster Stemmer   \n",
      "الجري               الجري               الجري               \n",
      "تجري                تجري                تجري                \n",
      "يجرون               يجرون               يجرون               \n",
      "جري                 جري                 جري                 \n",
      "يجري                يجري                يجري                \n"
     ]
    }
   ],
   "source": [
    "words = ['الجري','تجري','يجرون','جري','يجري']\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in words:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,ps.stem(word),ls.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الجري\n",
      "تجري\n",
      "يجرون\n",
      "جري\n",
      "يجري\n"
     ]
    }
   ],
   "source": [
    "words = ['الجري','تجري','يجرون','جري','يجري']\n",
    "\n",
    "for word in words : \n",
    "    print(lemmatizer.lemmatize(word))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
